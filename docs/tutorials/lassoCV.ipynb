{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lassoCV\n",
    "\n",
    "This notebook demonstrates how to conduct Lasso with stratified K fold cross validation\n",
    "on the Calling Cards data.  \n",
    "\n",
    "## Pulling the data\n",
    "\n",
    "The calling cards data should now strictly be taken from data source 'brent_nf_cc'. All\n",
    "of the Mitra data has been reprocessed through the nf-core/callingcards:1.0.0 pipeline.  \n",
    "\n",
    "Where there are multiple replicates, they have been aggregated. The `deduplicate`\n",
    "parameter to `PromoterSetSigAPI()` selects aggregated data where it exists. Where\n",
    "there is a single passing replicate, that replicate is used.\n",
    "\n",
    "## Setup\n",
    "\n",
    "As usual, import the `yeastdnnexplorer` interface functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import patsy as pt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.base import BaseEstimator, clone\n",
    "\n",
    "from yeastdnnexplorer.interface import *\n",
    "\n",
    "# configure the logger to print to console\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "\n",
    "pss_api = PromoterSetSigAPI()\n",
    "expression_api = ExpressionAPI()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pull the deduplicated calling cards data\n",
    "\n",
    "This will pull all of the currently usable data. In the future, we will remove \n",
    "\"unreviewed\". This will take a minute or two as it will need to fetch all of the\n",
    "underlying data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pss_api.push_params(\n",
    "    {\n",
    "        \"source_name\": \"brent_nf_cc\",\n",
    "        \"deduplicate\": \"true\",\n",
    "        \"data_usable\": [\"unreviewed\", \"pass\"],\n",
    "    }\n",
    ")\n",
    "\n",
    "pss_res = await pss_api.read(retrieve_files=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pull the corresponding perturbation data\n",
    "\n",
    "In this case, we are pulling the McIsaac data. In order to label blacklisted genes,\n",
    "we'll need the shrunken data. For modelling, we will use the unshrunken data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expression_api.push_params(\n",
    "    {\n",
    "        \"regulator_symbol\": \",\".join(\n",
    "            pss_res.get(\"metadata\").regulator_symbol.unique().tolist()\n",
    "        ),\n",
    "        \"source_name\": \"mcisaac_oe\",\n",
    "        \"time\": \"15\",\n",
    "    }\n",
    ")\n",
    "\n",
    "expression_res_shrunken = await expression_api.read(retrieve_files=True)\n",
    "\n",
    "# this will add the effect_colname parameter to the expression API\n",
    "expression_api.push_params(\n",
    "    {\n",
    "        \"effect_colname\": \"log2_ratio\",\n",
    "    }\n",
    ")\n",
    "\n",
    "expression_res_unshrunken = await expression_api.read(retrieve_files=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform the data into a usable format for modelling\n",
    "\n",
    "Note that there are new functions, `metric_arrays` and'\n",
    "`negative_log_transform_by_pvalue_and_enrichment`. See the API section of this\n",
    "documentation for more details.  \n",
    "\n",
    "You will likely want to save the results of this cell so that you do not have to run\n",
    "the DB or transformation steps in future sessions, unless of course you need or want\n",
    "to update the your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the data to a more managable format using `metric_arrays`\n",
    "\n",
    "X = metric_arrays(\n",
    "    pss_res,\n",
    "    {\"poisson_pval\": np.min, \"callingcards_enrichment\": np.max},\n",
    ")\n",
    "\n",
    "Y = metric_arrays(\n",
    "    expression_res_unshrunken,\n",
    "    {\"effect\": np.max},\n",
    ")\n",
    "\n",
    "Y_shrunken = metric_arrays(\n",
    "    expression_res_shrunken,\n",
    "    {\"effect\": np.max},\n",
    ")\n",
    "\n",
    "# define a set of common genes between X and y\n",
    "common_genes = X[\"poisson_pval\"].index.intersection(\n",
    "    Y.get(\"effect\", pd.DataFrame()).index\n",
    ")\n",
    "\n",
    "# binarize the Y.get(\"effect\") DataFrame as True if the value is not 0\n",
    "# We wish to exclude any genes that are always unresponsive\n",
    "Y_binary = Y_shrunken.get(\"effect\", pd.DataFrame()).eq(0)\n",
    "\n",
    "# define blacklisted genes as those records where they are labeled \"unresponsive\"\n",
    "# in all columns\n",
    "blacklisted_genes = Y_binary[Y_binary.all(axis=1)].index.intersection(common_genes)\n",
    "\n",
    "# remove the blacklist genes from Y and retain only common genes\n",
    "Y_filtered = Y.get(\"effect\", pd.DataFrame()).loc[common_genes].drop(blacklisted_genes)\n",
    "\n",
    "# remove the blacklisted_genes for X and retain only the common genes\n",
    "X_filtered = {}\n",
    "for key in X.keys():\n",
    "    X_filtered[key] = X[key].loc[common_genes].drop(blacklisted_genes)\n",
    "\n",
    "# Next, transform the X object into a predictors_df using the shifted negative log rank\n",
    "# transformation. See `negative_log_transform_by_pvalue_and_enrichment` for more\n",
    "# details\n",
    "scores_list = [\n",
    "    negative_log_transform_by_pvalue_and_enrichment(\n",
    "        X_filtered[\"poisson_pval\"].loc[:, i],\n",
    "        X_filtered[\"callingcards_enrichment\"].loc[:, i],\n",
    "    )\n",
    "    for i in X_filtered[\"poisson_pval\"].columns\n",
    "]\n",
    "\n",
    "# Convert the list of scores into a DataFrame\n",
    "predictors_df = pd.DataFrame(scores_list).T\n",
    "\n",
    "# Set the index and columns to match X_filtered[\"poisson_pval\"]\n",
    "predictors_df.index = X_filtered[\"poisson_pval\"].index\n",
    "predictors_df.columns = X_filtered[\"poisson_pval\"].columns\n",
    "\n",
    "\n",
    "# conduct a similar shifted negative log rank transformation on the Y values\n",
    "Y_filtered_ranked = Y_filtered.rank(ascending=False, method=\"average\")\n",
    "Y_filtered_transformed = Y_filtered_ranked.apply(shifted_negative_log_ranks, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model the data, one TF at a time\n",
    "\n",
    "It is the case that there are some TFs with replicates in the expression data. Those \n",
    "can be combined in the future, but for now I wanted to leave them and examine\n",
    "the modeling results. The important feature to notice in this section is that the \n",
    "modelling is a loop over the column names of `Y`. This is embarrassingly parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stratification_classification(binding_vector: pd.Series, perturbation_vector: pd.Series) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Bin the binding and perturbation data and create groups for stratified k folds\n",
    "    \"\"\"\n",
    "    \n",
    "    # Rank genes by binding and perturbation scores\n",
    "    binding_rank = pd.Series(binding_vector).rank(method='min', ascending=False).values\n",
    "    perturbation_rank = pd.Series(perturbation_vector).rank(method='min', ascending=False).values\n",
    "    \n",
    "    # Define bins for classification\n",
    "    bins = [0, 8, 64, 512, np.inf]\n",
    "    labels = [1, 2, 3, 4]\n",
    "    \n",
    "    # Bin genes based on binding and perturbation ranks\n",
    "    binding_bin = pd.cut(binding_rank, bins=bins, labels=labels, right=True).astype(int)\n",
    "    perturbation_bin = pd.cut(perturbation_rank, bins=bins, labels=labels, right=True).astype(int)\n",
    "    \n",
    "    # Generate a combined classification value\n",
    "    return (binding_bin - 1) * 4 + perturbation_bin\n",
    "\n",
    "def interaction_modeling(\n",
    "    colname: str,\n",
    "    Y_filtered_transformed: pd.DataFrame,\n",
    "    predictors_df: pd.DataFrame,\n",
    "    formula: str = None,\n",
    "    estimator: BaseEstimator = LassoCV()\n",
    ") -> BaseEstimator:\n",
    "    \"\"\"\n",
    "    Model interaction terms with a specified target column from Y_filtered_transformed\n",
    "    and predictors in predictors_df, using LassoCV with stratified cross-validation.\n",
    "\n",
    "    :param colname: The column of the response data to use as a model. Note: it is\n",
    "        assumed that the columns of Y_filtered_transformed are a subset of\n",
    "        predictors_df\n",
    "    :param Y_filtered_transformed: The transformed target DataFrame\n",
    "    :param predictors_df: The predictors DataFrame\n",
    "    :param formula: The formula to use for the interaction model. If None, the formula\n",
    "    :param estimator: The estimator to use for fitting the model. It must have a `cv`\n",
    "        attribute that can be set with a list of StratifiedKFold splits\n",
    "\n",
    "    :return: The LassoCV model\n",
    "\n",
    "    :raises ValueError: If the estimator does not have a `cv` attribute\n",
    "    \"\"\"\n",
    "    # Verify estimator has a `cv` attribute\n",
    "    if not hasattr(estimator, 'cv'):\n",
    "        raise ValueError(\"The estimator must support a `cv` parameter.\")\n",
    "\n",
    "    # Step 1: Create a temporary DataFrame and add the target column as `<colname>_LRR`\n",
    "    tmp_df = predictors_df.copy()\n",
    "    tmp_df[f\"{colname}_LRR\"] = Y_filtered_transformed[colname]\n",
    "\n",
    "    # Step 2: Drop the row where index matches `colname` if it exists\n",
    "    if colname in tmp_df.index:\n",
    "        tmp_df = tmp_df.drop(index=colname)\n",
    "\n",
    "    # Step 3: Define the interaction formula\n",
    "    if formula is None:\n",
    "        interaction_terms = \" + \".join(\n",
    "            [\n",
    "                f\"{colname}:{other_col}\"\n",
    "                for other_col in predictors_df.columns\n",
    "                if other_col != colname\n",
    "            ]\n",
    "        )\n",
    "        formula = f\"{colname}_LRR ~ {colname} + {interaction_terms}\"\n",
    "\n",
    "    # Step 4: Generate X, y matrices with patsy\n",
    "    y, X = pt.dmatrices(formula, tmp_df, return_type=\"dataframe\")\n",
    "\n",
    "    # Step 5: Generate bins for stratified k-fold cross-validation\n",
    "    classes = stratification_classification(X[colname], y.values.ravel())\n",
    "    \n",
    "    # Step 6: Initialize StratifiedKFold for stratified splits\n",
    "    skf = StratifiedKFold(n_splits=4, shuffle=True, random_state=42)\n",
    "    folds = list(skf.split(X, classes))\n",
    "\n",
    "    # Clone the estimator and set the `cv` attribute with predefined folds\n",
    "    model = clone(estimator)\n",
    "    model.cv = folds\n",
    "\n",
    "    # Step 7: Fit the model using the custom cross-validation folds\n",
    "    model.fit(X, y.values.ravel())\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling per TF\n",
    "\n",
    "`estimator` can be any sci-kit learn estimator that has a `cv` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_estimator = LassoCV(max_iter=10000)\n",
    "\n",
    "lasso_model = interaction_modeling(\"CBF1\", Y_filtered_transformed, predictors_df, estimator=lasso_estimator)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
