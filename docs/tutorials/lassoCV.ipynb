{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lassoCV\n",
    "\n",
    "This notebook demonstrates how to conduct Lasso with stratified K fold cross validation\n",
    "on the Calling Cards data.  \n",
    "\n",
    "## Pulling the data\n",
    "\n",
    "The calling cards data should now strictly be taken from data source 'brent_nf_cc'. All\n",
    "of the Mitra data has been reprocessed through the nf-core/callingcards:1.0.0 pipeline.  \n",
    "\n",
    "Where there are multiple replicates, they have been aggregated. The `deduplicate`\n",
    "parameter to `PromoterSetSigAPI()` selects aggregated data where it exists. Where\n",
    "there is a single passing replicate, that replicate is used.\n",
    "\n",
    "## Setup\n",
    "\n",
    "As usual, import the `yeastdnnexplorer` interface functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import patsy as pt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.base import BaseEstimator, clone\n",
    "from sklearn.utils import resample\n",
    "\n",
    "from yeastdnnexplorer.interface import *\n",
    "\n",
    "# configure the logger to print to console\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "\n",
    "pss_api = PromoterSetSigAPI()\n",
    "expression_api = ExpressionAPI()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pull the deduplicated calling cards data\n",
    "\n",
    "This will pull all of the currently usable data. In the future, we will remove \n",
    "\"unreviewed\". This will take a minute or two as it will need to fetch all of the\n",
    "underlying data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pss_api.push_params(\n",
    "    {\n",
    "        \"source_name\": \"brent_nf_cc\",\n",
    "        \"deduplicate\": \"true\",\n",
    "        \"data_usable\": [\"unreviewed\", \"pass\"],\n",
    "    }\n",
    ")\n",
    "\n",
    "pss_res = await pss_api.read(retrieve_files=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pull the corresponding perturbation data\n",
    "\n",
    "In this case, we are pulling the McIsaac data. In order to label blacklisted genes,\n",
    "we'll need the shrunken data. For modelling, we will use the unshrunken data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expression_api.push_params(\n",
    "    {\n",
    "        \"regulator_symbol\": \",\".join(\n",
    "            pss_res.get(\"metadata\").regulator_symbol.unique().tolist()\n",
    "        ),\n",
    "        \"source_name\": \"mcisaac_oe\",\n",
    "        \"time\": \"15\",\n",
    "    }\n",
    ")\n",
    "\n",
    "expression_res_shrunken = await expression_api.read(retrieve_files=True)\n",
    "\n",
    "# this will add the effect_colname parameter to the expression API\n",
    "expression_api.push_params(\n",
    "    {\n",
    "        \"effect_colname\": \"log2_ratio\",\n",
    "    }\n",
    ")\n",
    "\n",
    "expression_res_unshrunken = await expression_api.read(retrieve_files=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform the data into a usable format for modelling\n",
    "\n",
    "Note that there are new functions, `metric_arrays` and'\n",
    "`negative_log_transform_by_pvalue_and_enrichment`. See the API section of this\n",
    "documentation for more details.  \n",
    "\n",
    "You will likely want to save the results of this cell so that you do not have to run\n",
    "the DB or transformation steps in future sessions, unless of course you need or want\n",
    "to update the your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the data to a more managable format using `metric_arrays`\n",
    "\n",
    "X = metric_arrays(\n",
    "    pss_res,\n",
    "    {\"poisson_pval\": np.min, \"callingcards_enrichment\": np.max},\n",
    ")\n",
    "\n",
    "Y = metric_arrays(\n",
    "    expression_res_unshrunken,\n",
    "    {\"effect\": np.max},\n",
    ")\n",
    "\n",
    "Y_shrunken = metric_arrays(\n",
    "    expression_res_shrunken,\n",
    "    {\"effect\": np.max},\n",
    ")\n",
    "\n",
    "# define a set of common genes between X and y\n",
    "common_genes = X[\"poisson_pval\"].index.intersection(\n",
    "    Y.get(\"effect\", pd.DataFrame()).index\n",
    ")\n",
    "\n",
    "# binarize the Y.get(\"effect\") DataFrame as True if the value is not 0\n",
    "# We wish to exclude any genes that are always unresponsive\n",
    "Y_binary = Y_shrunken.get(\"effect\", pd.DataFrame()).eq(0)\n",
    "\n",
    "# define blacklisted genes as those records where they are labeled \"unresponsive\"\n",
    "# in all columns\n",
    "blacklisted_genes = Y_binary[Y_binary.all(axis=1)].index.intersection(common_genes)\n",
    "\n",
    "# remove the blacklist genes from Y and retain only common genes\n",
    "Y_filtered = Y.get(\"effect\", pd.DataFrame()).loc[common_genes].drop(blacklisted_genes)\n",
    "\n",
    "# remove the blacklisted_genes for X and retain only the common genes\n",
    "X_filtered = {}\n",
    "for key in X.keys():\n",
    "    X_filtered[key] = X[key].loc[common_genes].drop(blacklisted_genes)\n",
    "\n",
    "# Next, transform the X object into a predictors_df using the shifted negative log rank\n",
    "# transformation. See `negative_log_transform_by_pvalue_and_enrichment` for more\n",
    "# details\n",
    "scores_list = [\n",
    "    negative_log_transform_by_pvalue_and_enrichment(\n",
    "        X_filtered[\"poisson_pval\"].loc[:, i],\n",
    "        X_filtered[\"callingcards_enrichment\"].loc[:, i],\n",
    "    )\n",
    "    for i in X_filtered[\"poisson_pval\"].columns\n",
    "]\n",
    "\n",
    "# Convert the list of scores into a DataFrame\n",
    "predictors_df = pd.DataFrame(scores_list).T\n",
    "\n",
    "# Set the index and columns to match X_filtered[\"poisson_pval\"]\n",
    "predictors_df.index = X_filtered[\"poisson_pval\"].index\n",
    "predictors_df.columns = X_filtered[\"poisson_pval\"].columns\n",
    "\n",
    "\n",
    "# conduct a similar shifted negative log rank transformation on the Y values\n",
    "Y_filtered_ranked = Y_filtered.rank(ascending=False, method=\"average\")\n",
    "Y_filtered_transformed = Y_filtered_ranked.apply(shifted_negative_log_ranks, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions\n",
    "\n",
    "These are drafts of functions to run LassoCV and bootstrap_lassoCV. They parse the\n",
    "data into a response dataframe (single column dataframe) and predictor dataframe using\n",
    "a formula via patsy. There is a function that will use the data to generate the bins\n",
    "for StratifiedKFold. `interaction_modeling` runs the LassoCV over those folds (though\n",
    "any estimator with a `cv` method would work in this function). Finally,\n",
    "`boostrap_lassoCV` is an implementation of the bootstrap for the Lasso based on \n",
    "Hastie et al.\n",
    "[Statistical Learning with Sparsity](https://hastie.su.domains/StatLearnSparsity/). See\n",
    "chapter 6.2.  \n",
    "\n",
    "These functions and their design is not yet clear in my mind, hence including them here\n",
    "rather than in the source code. The purpose will be to move them into source once we\n",
    "have a clear sense of how they will be used.\n",
    "\n",
    "**NOTE**: There is currently an expectation in these functions that there may be\n",
    "replicates in the response data with the suffix _rep\\d, eg `CBF1_rep2`, but that there\n",
    "are no replicates in the binding data. This is not well handled and needs to be treated\n",
    "with care."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import Tuple, Union, Dict\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "def generate_modeling_data(\n",
    "    colname: str,\n",
    "    response_df: pd.DataFrame,\n",
    "    predictors_df: pd.DataFrame,\n",
    "    formula: str = None\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Generate the response and predictor data\n",
    "\n",
    "    :param colname: The column name to use as the response variable. This column name\n",
    "        should exist in `response_df` and predictors_df\n",
    "    :param response_df: The transformed response variable DataFrame\n",
    "    :param predictors_df: The predictors DataFrame\n",
    "\n",
    "    :return: A tuple of the response variable DataFrame and the predictors DataFrame\n",
    "\n",
    "    :raises ValueError: If `colname` does not exists in `predictors_df`\n",
    "    :raises ValueError: if all of the columns in `response_df` are not present in\n",
    "        `predictors_df`\n",
    "    \"\"\"\n",
    "    # Validate input\n",
    "    if colname not in response_df.columns:\n",
    "        raise ValueError(f\"The column {colname} does not exist in the response DataFrame.\")\n",
    "    # get any column in response_df that is not in predictors_df.\n",
    "    # If there are any, raise an error and print the columns that are not in predictors_df\n",
    "    # Remove the pattern \"_rep\\d+\" from the column names\n",
    "    response_columns = response_df.columns.str.replace(r'_rep\\d+', '', regex=True)\n",
    "    # Find the missing columns, if any\n",
    "    missing_cols = response_columns.difference(predictors_df.columns)\n",
    "    if not missing_cols.empty:\n",
    "        raise ValueError(f\"The following columns are missing from the predictors DataFrame: {missing_cols}\")\n",
    "\n",
    "    tmp_df = predictors_df.copy()\n",
    "    tmp_df[f\"{colname}_LRR\"] = response_df[colname]\n",
    "\n",
    "    # Step 2: Drop the row where index matches `colname` if it exists\n",
    "    if colname in tmp_df.index:\n",
    "        tmp_df = tmp_df.drop(index=colname)\n",
    "\n",
    "    # Step 3: Define the interaction formula\n",
    "    perturbed_tf = re.sub(r'_rep\\d+', '', colname)\n",
    "    if formula is None:\n",
    "        interaction_terms = \" + \".join(\n",
    "            [\n",
    "                f\"{perturbed_tf}:{other_col}\"\n",
    "                for other_col in predictors_df.columns\n",
    "                if other_col != perturbed_tf\n",
    "            ]\n",
    "        )\n",
    "        formula = f\"{colname}_LRR ~ {perturbed_tf} + {interaction_terms}\"\n",
    "\n",
    "    # Step 4: Generate X, y matrices with patsy\n",
    "    y, X = pt.dmatrices(formula, tmp_df, return_type=\"dataframe\")\n",
    "\n",
    "    y.columns = y.columns.str.replace('_LRR', '')\n",
    "\n",
    "    return y, X\n",
    "\n",
    "def stratification_classification(binding_vector: pd.Series, perturbation_vector: pd.Series) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Bin the binding and perturbation data and create groups for stratified k folds\n",
    "    \"\"\"\n",
    "    \n",
    "    # Rank genes by binding and perturbation scores\n",
    "    binding_rank = pd.Series(binding_vector).rank(method='min', ascending=False).values\n",
    "    perturbation_rank = pd.Series(perturbation_vector).rank(method='min', ascending=False).values\n",
    "    \n",
    "    # Define bins for classification\n",
    "    bins = [0, 8, 64, 512, np.inf]\n",
    "    labels = [1, 2, 3, 4]\n",
    "    \n",
    "    # Bin genes based on binding and perturbation ranks\n",
    "    binding_bin = pd.cut(binding_rank, bins=bins, labels=labels, right=True).astype(int)\n",
    "    perturbation_bin = pd.cut(perturbation_rank, bins=bins, labels=labels, right=True).astype(int)\n",
    "    \n",
    "    # Generate a combined classification value\n",
    "    return (binding_bin - 1) * 4 + perturbation_bin\n",
    "\n",
    "\n",
    "def interaction_modeling(y:pd.DataFrame, X:pd.DataFrame, estimator: BaseEstimator = LassoCV()) -> BaseEstimator:\n",
    "    \"\"\"\n",
    "\n",
    "    :param y: The response variable to use for modeling. This should be a single column.\n",
    "        See `generate_modeling_data()`\n",
    "    :param X: The predictors to use for modeling. This should be an NxP DataFrame where\n",
    "        N == len(y) and P is the number of predictors. See `generate_modeling_data()`\n",
    "    :param estimator: The estimator to use for fitting the model. It must have a `cv`\n",
    "        attribute that can be set with a list of StratifiedKFold splits\n",
    "\n",
    "    :return: The LassoCV model\n",
    "\n",
    "    :raises ValueError: if y is not a single column DataFrame\n",
    "    :raises ValueError: if X is not a DataFrame with 1 or more columns, or the number\n",
    "        of rows in y does not match the number of rows in X\n",
    "    :raises ValueError: If the estimator does not have a `cv` attribute\n",
    "    \"\"\"\n",
    "    # Validate data\n",
    "    if not isinstance(y, pd.DataFrame):\n",
    "        raise ValueError(\"The response variable y must be a DataFrame.\")\n",
    "    if y.shape[1] != 1:\n",
    "        raise ValueError(\"The response variable y must be a single column DataFrame.\")\n",
    "    if not isinstance(X, pd.DataFrame):\n",
    "        raise ValueError(\"The predictors X must be a DataFrame.\")\n",
    "    if X.shape[0] != y.shape[0]:\n",
    "        raise ValueError(\"The number of rows in X must match the number of rows in y.\")\n",
    "\n",
    "    # Verify estimator has a `cv` attribute\n",
    "    if not hasattr(estimator, 'cv'):\n",
    "        raise ValueError(\"The estimator must support a `cv` parameter.\")\n",
    "\n",
    "    # Step 5: Generate bins for stratified k-fold cross-validation\n",
    "    response_colname = re.sub(r'_rep\\d+', '', y.columns[0])\n",
    "    classes = stratification_classification(X.loc[:, response_colname].values.ravel(), y.values.ravel())\n",
    "    \n",
    "    # Step 6: Initialize StratifiedKFold for stratified splits\n",
    "    skf = StratifiedKFold(n_splits=4, shuffle=True, random_state=42)\n",
    "    folds = list(skf.split(X, classes))\n",
    "\n",
    "    # Clone the estimator and set the `cv` attribute with predefined folds\n",
    "    model = clone(estimator)\n",
    "    model.cv = folds\n",
    "\n",
    "    # Step 7: Fit the model using the custom cross-validation folds\n",
    "    model.fit(X, y.values.ravel())\n",
    "\n",
    "    return model\n",
    "\n",
    "def bootstrap_lassoCV(\n",
    "    n_bootstraps: int = 1000,\n",
    "    ci_percentile: float = 95,\n",
    "    return_raw: bool = False,\n",
    "    **kwargs\n",
    ") -> Tuple[BaseEstimator, Union[np.ndarray, Dict[str, Tuple[float, float]]]]:\n",
    "    \"\"\"\n",
    "    Perform bootstrap resampling to generate confidence intervals for\n",
    "    Lasso coefficients. See 6.2 in https://hastie.su.domains/StatLearnSparsity/ -- this\n",
    "    is an implementation of the algorithm described in that section.\n",
    "\n",
    "    :param n_bootstraps: The number of bootstrap samples to generate.\n",
    "    :param ci_percentile: The percentile for the confidence interval (e.g., 95 for 95% CI).\n",
    "    :param kwargs: The required arguments to `interaction_modeling` must be\n",
    "        passed as keyword arguments to this function\n",
    "\n",
    "    :return: a tuple where the first element is the model fitted to the whole data\n",
    "        and the second element is either the raw bootstrap coefficients from each\n",
    "        iteration, if return_raw is True, or a dictionary of confidence intervals\n",
    "        with keys `coef_i` and values as a tuple of (lower_bound, upper_bound)\n",
    "        according to `ci_percentile`.\n",
    "    \"\"\"\n",
    "    # extract the interactor_model arguments and validate\n",
    "    y = kwargs.get(\"y\")\n",
    "    X = kwargs.get(\"X\")\n",
    "    estimator = kwargs.get(\"estimator\")\n",
    "    if any(x is None for x in [y, X, estimator]):\n",
    "        raise ValueError(\"The arguments for bootstrap_lassoCV(), \"\n",
    "                        \"'y', 'X', and 'estimator' must be passed.\")\n",
    "\n",
    "    # Fit the model to the data using CV to find the optimal alpha. Note that this is\n",
    "    # used in the Lasso estimator for the bootstrap iterations\n",
    "    fitted_model = interaction_modeling(y,\n",
    "                                        X,\n",
    "                                        estimator)\n",
    "\n",
    "    # Placeholder for storing bootstrap coefficients\n",
    "    bootstrap_coefs = []\n",
    "    \n",
    "    # Run bootstrap iterations\n",
    "    for _ in range(n_bootstraps):\n",
    "        # Resample the data\n",
    "        Y_resampled = resample(y)\n",
    "        X_resampled = X.loc[Y_resampled.index]\n",
    "\n",
    "        bootstrap_model_i = Lasso(alpha=fitted_model.alpha_, max_iter=10000)\n",
    "        bootstrap_model_i.fit(X_resampled, Y_resampled)\n",
    "\n",
    "        # Extract and store coefficients\n",
    "        bootstrap_coefs.append(bootstrap_model_i.coef_)\n",
    "\n",
    "    # Convert to array for easier manipulation\n",
    "    bootstrap_coefs = np.array(bootstrap_coefs)\n",
    "\n",
    "    # Compute confidence intervals\n",
    "    lower_bound = (100 - ci_percentile) / 2\n",
    "    upper_bound = 100 - lower_bound\n",
    "    ci_bounds = np.percentile(bootstrap_coefs, [lower_bound, upper_bound], axis=0)\n",
    "\n",
    "    # Organize the confidence intervals into a dictionary\n",
    "    confidence_intervals = {\n",
    "        f\"coef_{i}\": (ci_bounds[0, i], ci_bounds[1, i]) for i in range(bootstrap_coefs.shape[1])\n",
    "    }\n",
    "\n",
    "    if return_raw:\n",
    "        fitted_model, bootstrap_coefs\n",
    "    else:\n",
    "        return fitted_model, confidence_intervals\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling per TF\n",
    "\n",
    "This demonstrates the usage on a single TF. The TFs are the columns in both the \n",
    "response and predictor dataframes. To run this on all TFs, you would simply iterate\n",
    "over the columns of the response DF. This can be done in parallel very easily on the\n",
    "cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is here simply to emphasize that there are some RESPONSE variables that have\n",
    "# replicates. They are retained in the response data in order to explore\n",
    "# 1. how correlated the response replicates are and b) how different the models\n",
    "# generated are\n",
    "cbf1_y, cbf1_X = generate_modeling_data(\"CBF1_rep2\", Y_filtered_transformed, predictors_df)\n",
    "\n",
    "lassoCV_estimator = LassoCV(max_iter=10000)\n",
    "\n",
    "# n_boostraps set to 10 for demo purposes. The Hastie et al. book used 1000\n",
    "# 10 bootstraps takes ~ 20 seconds\n",
    "lasso_model = bootstrap_lassoCV(y=cbf1_y, X=cbf1_X, estimator=lassoCV_estimator, n_bootstraps=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
