{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure the logger to print to console\n",
    "import logging\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from yeastdnnexplorer.ml_models.lasso_modeling import (\n",
    "    generate_modeling_data,\n",
    "    get_significant_predictors,\n",
    "    stratified_cv_r2,\n",
    "    get_interactor_importance,\n",
    "    OLSFeatureSelector\n",
    "    )\n",
    "\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "\n",
    "random.seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interactor Modeling Workflow 1\n",
    "\n",
    "This tutorial describes a process of modeling perturbation response by binding data\n",
    "with the goal of discovering a meaningful set of interactor terms. More specifically,\n",
    "we start with the following model:\n",
    "\n",
    "$$\n",
    "tf_{perturbed} \\sim tf_{perturbed} + tf_{perturbed}:tf_{2} + tf_{perturbed}:tf_{2} + ... + max(non\\ perturbed\\ binding)\n",
    "$$\n",
    "\n",
    "Where the response variable is the $tf_{perturbed}$ perturbation response, and the\n",
    "predictor variables are binding data (e.g., calling card experiments). Predictor\n",
    "terms such as $tf_{perturbed}:tf_{2}$ represent the interaction between the\n",
    "$tf_{perturbed}$ binding and the binding of another transcription factor. The final\n",
    "term, $\\max(\\text{non-perturbed binding})$, is defined as the maximum binding score\n",
    "for each gene, excluding $tf_{perturbed}$. This term is included to mitigate the\n",
    "effect of outlier genes which may have high binding scores across multiple\n",
    "transcription factors, potentially distorting the model.\n",
    "\n",
    "We assume that the actual relationship between the perturbation response and the\n",
    "binding data is sparse and use the following steps to identify significant terms.\n",
    "These terms represent a set of TFs which, when considered as interactors with the\n",
    "perturbed TF, improve the inferred relationship between the binding and perturbation\n",
    "data.\n",
    "\n",
    "\n",
    "## Interactor sparse modeling\n",
    "\n",
    "1. We currently have two methods to produce a set of what we deem to be significant\n",
    "initial parameters. In the first, we use the bootstrap on a 4-fold cross-validated\n",
    "Lasso model. In this case, we consider a coefficient significant if the confidence\n",
    "interval generated by the bootstrap is far enough away from 0. The second method is\n",
    "to perform 4-fold cross validated Lasso once, and then with the non-zero coefficients, \n",
    "iteratively use OLS modeling with a p-value threshold to drop insignificant predictors. \n",
    "With either method, we produce two variations of this model:\n",
    "\n",
    "    1. A model trained using all available data. For the bootstrapped LassoCV, we\n",
    "    accept a coefficient if its 99.8% confidence interval does not include 0.0. For\n",
    "    the OLS reduction method, we accept a predictor when its p-value is <= 0.001\n",
    "    \n",
    "    2. A model trained using only the top 10% of data based on the binding\n",
    "    score of the perturbed TF. For the bootstrapped LassoCV, we\n",
    "    accept a coefficient if its 90.0% confidence interval does not include 0.0. For\n",
    "    the OLS reduction method, we accept a predictor when its p-value is <= 0.01\n",
    "\n",
    "1. We intersect predictors that result from steps 1.1 and 1.2\n",
    "\n",
    "1. With this set of predictors, we then test whether the interactor is a significantly\n",
    "better predictor than the corresponding main effect. We do this by individually\n",
    "swapping out the interactor term for its main effect and comparing the average\n",
    "r-squared of OLS models across 4 folds. When this process is complete, we adjust our\n",
    "predictors by replacing interactors with main effects where the main effect was deemed\n",
    "better.\n",
    "\n",
    "1. Finally, we report, as significant interactors, the interaction terms which have\n",
    "survived the steps above. We use the average R-squared achieved by this model and\n",
    "compare it to the average R-squared achieved by the univariate counterpart (the\n",
    "response TF predicted solely by its main effect). We would like to create a boxplot\n",
    "of this comparison across all TFs to see how this pipeline affects model performance\n",
    "in explaining variance in contrast to the simple univariate model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***NOTE***: To generate the `response_df` and `predictors_df` below, see the first six \n",
    "cells in the LassoCV tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_df = pd.read_csv(\"~/htcf_local/lasso_bootstrap/erics_tfs/response_dataframe_20241105.csv\", index_col=0)\n",
    "predictors_df = pd.read_csv(\"~/htcf_local/lasso_bootstrap/erics_tfs/predictors_dataframe_20241105.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will demonstrate both methods of identifying a significant set of interactor terms\n",
    "using the TF Cbf1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_tf = 'CBF1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Find significant predictors using all of the data\n",
    "\n",
    "The function `get_significant_predictors()` is capable of choosing significant\n",
    "perdictors using two methods: either by the bootstrap with LassoCV, or by a single\n",
    "LassoCV model and subsequently choosing only significant coefficients through\n",
    "iterative OLS modeling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Significant coefficients for 99.8, where intervals are entirely above or below ±0.0:\n",
      "CBF1:SWI6: (-0.14416455697760802, -0.009884405236493667)\n",
      "CBF1:RGM1: (0.014576695345595624, 0.16062245178888665)\n",
      "CBF1:ARG81: (-0.21487429483287668, -0.03345111234665463)\n",
      "CBF1:MET28: (0.07821562304993974, 0.20784574601671219)\n",
      "CBF1:AZF1: (-0.15416577155959094, -0.026421884027885402)\n",
      "CBF1:GAL4: (0.09086284981352438, 0.3185129462792361)\n",
      "CBF1:MSN2: (0.10065808506838035, 0.2776672633042757)\n",
      "max_lrb: (0.0018092836372706933, 0.0983814591861913)\n",
      "Significant coefficients for 90.0, where intervals are entirely above or below ±0.0:\n",
      "CBF1:MET28: (0.06998122210410229, 0.2088974100580522)\n"
     ]
    }
   ],
   "source": [
    "# step 1.1 with the bootstrapped lassoCV\n",
    "bootstrap_lasso_all_data = get_significant_predictors(\n",
    "    \"bootstrap_lassocv\",\n",
    "    model_tf,\n",
    "    response_df,\n",
    "    predictors_df,\n",
    "    ci_percentile=99.8,\n",
    "    n_bootstraps=100,\n",
    "    add_max_lrb=True)\n",
    "\n",
    "# step 1.2 with the bootstrapped lassoCV\n",
    "bootstrap_lasso_top10 = get_significant_predictors(\n",
    "    \"bootstrap_lassocv\",\n",
    "    model_tf,\n",
    "    response_df,\n",
    "    predictors_df,\n",
    "    quantile_threshold=0.1,\n",
    "    ci_percentile=90.0,\n",
    "    n_bootstraps=100,\n",
    "    add_max_lrb=True)\n",
    "\n",
    "# step 1.1 with with the lassoCV method (the ols reduction comes later)\n",
    "lassocv_ols_all_data = get_significant_predictors(\n",
    "    \"lassocv_ols\",\n",
    "    model_tf,\n",
    "    response_df,\n",
    "    predictors_df,\n",
    "    add_max_lrb=True)\n",
    "\n",
    "# step 1.2 with with the lassoCV method (the ols reduction comes later)\n",
    "lassocv_ols_top10 = get_significant_predictors(\n",
    "    \"lassocv_ols\",\n",
    "    model_tf,\n",
    "    response_df,\n",
    "    predictors_df,\n",
    "    add_max_lrb=True,\n",
    "    quantile_threshold=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2\n",
    "\n",
    "We next need to intersect the significant coefficients (see definitions above) in both\n",
    "models. In this case, one interactors survives (note that there are only 100\n",
    "bootstraps in this example in the interest of speed for the tutorial. We recommend no \n",
    "less than 1000 in practice)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrap Lasso Intersect Coefs: {'CBF1:MET28'}\n",
      "LassoCV Intersect Coefs: {'CBF1:MET28', 'CBF1:SKN7', 'CBF1:SWI6', 'CBF1:AZF1'}\n"
     ]
    }
   ],
   "source": [
    "bootstrap_lassocv_final_features = (set(bootstrap_lasso_all_data['sig_coefs'].keys())\n",
    "                                   .intersection(set(bootstrap_lasso_top10['sig_coefs'].keys())))\n",
    "\n",
    "print(f\"Bootstrap Lasso Intersect Coefs: {bootstrap_lassocv_final_features}\")\n",
    "\n",
    "# these are not the final features from this method!\n",
    "lassocv_ols_intersect_coefs = (set(lassocv_ols_all_data['sig_coefs'].keys())\n",
    "                                 .intersection(set(lassocv_ols_top10['sig_coefs'].keys())))\n",
    "\n",
    "print(f\"LassoCV Intersect Coefs: {lassocv_ols_intersect_coefs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OLS reduction only\n",
    "\n",
    "For the LassoCV result, we go through another round of reduction. Using the intersect\n",
    "of the LassoCV result, we consider iteratively use this set of predictors with an OLS\n",
    "model. At eac iteration, if a predictor's pvalue falls below a given threshold, it is\n",
    "removed from the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Significant features in all data: ['CBF1:MET28', 'CBF1:SWI6']\n",
      "All data model summary:\n",
      "                 coef   std_err          t        pvalue\n",
      "const       0.421557  0.006203  67.964194  0.000000e+00\n",
      "CBF1:MET28  0.138165  0.012179  11.344703  1.547294e-29\n",
      "CBF1:SWI6  -0.104014  0.015297  -6.799570  1.148398e-11\n",
      "Significant features in top 10: ['CBF1:MET28']\n",
      "Top 10 model summary:\n",
      "                 coef   std_err          t        pvalue\n",
      "const       0.441569  0.029296  15.072468  6.710796e-44\n",
      "CBF1:MET28  0.083300  0.016322   5.103594  4.451965e-07\n",
      "LassoCV OLS Final Features: {'CBF1:MET28'}\n"
     ]
    }
   ],
   "source": [
    "# Initialize the selector\n",
    "selector_all = OLSFeatureSelector(p_value_threshold=0.001)\n",
    "\n",
    "# Transform the data to select only significant features\n",
    "selector_all.refine_features(\n",
    "    lassocv_ols_all_data['predictors'][list(lassocv_ols_intersect_coefs)],\n",
    "    lassocv_ols_all_data['response'],)\n",
    "\n",
    "selector_top10 = OLSFeatureSelector(p_value_threshold=0.01)\n",
    "\n",
    "_ = selector_top10.refine_features(\n",
    "    lassocv_ols_top10['predictors'].loc[lassocv_ols_top10['response'].index, \n",
    "                                        list(lassocv_ols_intersect_coefs)],\n",
    "    lassocv_ols_top10['response'],)\n",
    "\n",
    "\n",
    "# Get significant features and the model summary\n",
    "print(\"Significant features in all data:\", selector_all.get_significant_features(drop_intercept=True))\n",
    "print(\"All data model summary:\\n\", selector_all.get_summary())\n",
    "\n",
    "print(\"Significant features in top 10:\", selector_top10.get_significant_features(drop_intercept=True))\n",
    "print(\"Top 10 model summary:\\n\", selector_top10.get_summary())\n",
    "\n",
    "lassocv_ols_final_features = set(selector_all.get_significant_features(drop_intercept=True)).intersection(\n",
    "    selector_top10.get_significant_features(drop_intercept=True)\n",
    ")\n",
    "print(f\"LassoCV OLS Final Features: {lassocv_ols_final_features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3\n",
    "\n",
    "We next implement the method which searches alternative models, which include the\n",
    "surviving interactor terms, with variations on substituing in the main effect. In this case, \n",
    "we have a single term. However, if we had more than one term, we would do the following for each surviving interactor term. The goal of this process, remember, is to generate a set of\n",
    "high confidence interactor terms for this TF. If the predictive power of the main effect\n",
    "is equivalent or better than a model with the interactor, we consider that a low\n",
    "confidence interactor effect. \n",
    "\n",
    "After identifying all interactor terms for which substituting in the main effect improves\n",
    "the average R-squared from CV, we drop these terms from our set of features. We then log the final average R-squared achieved by this model. In this case, no terms are dropped from testing the substitution of main effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chase/code/yeastdnnexplorer/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:737: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=4.\n",
      "  warnings.warn(\n",
      "/home/chase/code/yeastdnnexplorer/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:737: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=4.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# in this case, the lassocv_ols_final_features are the bootstrap_lassocv_final_features\n",
    "# are the same. I'm setting the lassocv_ols_final_features to the final_features for\n",
    "# the following steps.\n",
    "final_features = lassocv_ols_final_features\n",
    "\n",
    "# get the additional main effects which will be tested from the final_features\n",
    "main_effects = []\n",
    "for term in final_features:\n",
    "    if \":\" in term:\n",
    "        main_effects.append(term.split(\":\")[1])\n",
    "    else:\n",
    "        main_effects.append(term)\n",
    "\n",
    "# combine these main effects with the final_features\n",
    "interactor_terms_and_main_effects = list(final_features) + main_effects\n",
    "\n",
    "# generate a model matrix with the intersect terms and the main effects. This full\n",
    "# model will not be used for modeling -- subsets of the columns will be, however.\n",
    "_, full_X = generate_modeling_data(\n",
    "    model_tf,\n",
    "    response_df,\n",
    "    predictors_df,\n",
    "    formula = f\"~ {' + '.join(interactor_terms_and_main_effects)}\",\n",
    "    drop_intercept=False,\n",
    ")\n",
    "\n",
    "# Add the max_lrb column, just in case it is present in the final_predictors. In this\n",
    "# case, it is not.\n",
    "max_lrb = predictors_df.drop(columns=model_tf).max(axis=1)\n",
    "full_X['max_lrb'] = max_lrb\n",
    "\n",
    "# Currently, this function tests each interactor term in the final_features\n",
    "# with two variants by replacing the interaction term with the main effect only, and\n",
    "# with the main effect + interactor. If either of the variants has a higher avg\n",
    "# r-squared than the intersect_model, then that variant is returned. In this case,\n",
    "# the original final_features are the best model.\n",
    "full_avg_rsquared, interactor_results = get_interactor_importance(\n",
    "    lassocv_ols_all_data['response'],\n",
    "    full_X,\n",
    "    lassocv_ols_all_data['classes'],\n",
    "    final_features\n",
    ")\n",
    "\n",
    "# use the interactor_results to update the final_features\n",
    "for interactor_variant in interactor_results:\n",
    "    k = interactor_variant['interactor']\n",
    "    v = interactor_variant['variant']\n",
    "    final_features.remove(k)\n",
    "    final_features.add(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Comparing our final model to a univariate model\n",
    "\n",
    "In our last step, we take our reamining set of features from the end of Step 3, and now compare its performance to that of a univariate model where the response TF is predicted solely by its main effect. We will use the average R-squared achieved by 4-Fold CV on both models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The univariate average R-squared is: 0.0049704126329321585\n",
      "The final model average R-squared is 0.010517208487239943\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chase/code/yeastdnnexplorer/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:737: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=4.\n",
      "  warnings.warn(\n",
      "/home/chase/code/yeastdnnexplorer/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:737: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=4.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "avg_r2_univariate = stratified_cv_r2(\n",
    "    lassocv_ols_all_data['response'],\n",
    "    lassocv_ols_all_data['predictors'][[model_tf]],  \n",
    "    lassocv_ols_all_data['classes'],\n",
    ")\n",
    "\n",
    "final_model_avg_r_squared = stratified_cv_r2(\n",
    "    lassocv_ols_all_data['response'],\n",
    "    full_X[list(final_features)],\n",
    "    lassocv_ols_all_data['classes'],\n",
    ")\n",
    "\n",
    "print(f\"The univariate average R-squared is: {avg_r2_univariate}\")\n",
    "print(f\"The final model average R-squared is {final_model_avg_r_squared}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the final model we achieved through Workflow 1 demonstrates a higher average R-squared achieved by 4-fold CV. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
