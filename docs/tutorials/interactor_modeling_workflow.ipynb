{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure the logger to print to console\n",
    "from typing import Union\n",
    "import logging\n",
    "from matplotlib import pyplot as plt\n",
    "import re\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, clone\n",
    "from sklearn.linear_model import LassoCV, LinearRegression\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "\n",
    "\n",
    "from yeastdnnexplorer.ml_models.lasso_modeling import (\n",
    "    generate_modeling_data,\n",
    "    stratification_classification,\n",
    "    stratified_cv_modeling,\n",
    "    bootstrap_stratified_cv_modeling,\n",
    "    examine_bootstrap_coefficients)\n",
    "\n",
    "\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "\n",
    "random.seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interactor Modeling Workflow\n",
    "\n",
    "This tutorial describes a process of modeling perturbation response by binding data\n",
    "with the goal of discovering a meaningful set of interactor terms. More specifically,\n",
    "we start with the following model:\n",
    "\n",
    "$$\n",
    "tf_{perturbed} \\sim tf_{perturbed} + tf_{perturbed}:tf_{2} + tf_{perturbed}:tf_{2} + ... + max(non\\ perturbed\\ binding)\n",
    "$$\n",
    "\n",
    "Where the response variable is the $tf_{perturbed}$ perturbation response, and the\n",
    "predictor variables are binding data (e.g., calling card experiments). Predictor\n",
    "terms such as $tf_{perturbed}:tf_{2}$ represent the interaction between the\n",
    "$tf_{perturbed}$ binding and the binding of another transcription factor. The final\n",
    "term, $\\max(\\text{non-perturbed binding})$, is defined as the maximum binding score\n",
    "for each gene, excluding $tf_{perturbed}$. This term is included to mitigate the\n",
    "effect of outlier genes which may have high binding scores across multiple\n",
    "transcription factors, potentially distorting the model.\n",
    "\n",
    "We assume that the actual relationship between the perturbation response and the\n",
    "binding data is sparse and use the following steps to identify significant terms.\n",
    "These terms represent a set of TFs which, when considered as interactors with the\n",
    "perturbed TF, improve the inferred relationship between the binding and perturbation\n",
    "data.\n",
    "\n",
    "\n",
    "## Interactor sparse modeling\n",
    "\n",
    "1. First, we apply bootstrapping to a 4-fold cross-validated Lasso model. The folds\n",
    "are stratified based on the binding data domain of the perturbed TF, ensuring that\n",
    "each fold better represents the domain structure.\n",
    "\n",
    "    - We produce two variations of this model:\n",
    "        \n",
    "        1. A model trained using all available data.\n",
    "        \n",
    "        2. A model trained using only the top 10% of data based on the binding\n",
    "        score of the perturbed TF.\n",
    "\n",
    "1. For model `1.1`, we select coefficients whose 99.8% confidence interval does not\n",
    "include zero. For model `1.2`, we select coefficients whose 90.0% confidence interval\n",
    "does not include zero. We assume that, due to the non-linear relationship between\n",
    "perturbation response and binding, interaction effects are more pronounced in the\n",
    "top 10% of the data. By intersecting the coefficients from both models, we highlight\n",
    "those that are predictive across the full dataset.\n",
    "\n",
    "1. With this set of predictors, next create an OLS model using the same 4-fold\n",
    "stratified cross validation from which we calculated an average $R^2$. Next, for each\n",
    "interactor in the model, we produce two other cross validated OLS models, one by\n",
    "replacing the interactor with its corresponding main effect, and another that\n",
    "includes both the interaction term and the main effect. We note which of these\n",
    "variants yields the best average $R^2$.\n",
    "\n",
    "1. Finally, we report, as significant interactors, those interaction terms which, when \n",
    "retained in the model, improve the $R^2$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***NOTE***: To generate the `response_df` and `predictors_df` below, see the first six \n",
    "cells in the LassoCV tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_df = pd.read_csv(\"~/htcf_local/lasso_bootstrap/erics_tfs/response_dataframe_20241105.csv\", index_col=0)\n",
    "predictors_df = pd.read_csv(\"~/htcf_local/lasso_bootstrap/erics_tfs/predictors_dataframe_20241105.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Find significant predictors using all of the data\n",
    "\n",
    "The function `get_significant_predictors()` is a wrapper of the lassoCV bootstrap \n",
    "protocol described in the LassoCV notebook. It allows using the same code to produce\n",
    "both the 'all data' (step 1.1) and 'top 10%' models (step 1.2), and returns the \n",
    "significant coefficients as described in the protocol above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Significant coefficients for 99.8, where intervals are entirely above or below ±0.0:\n",
      "CBF1:SWI6: (-0.1485865745733975, -0.013427250137445276)\n",
      "CBF1:RGM1: (0.019074716568397616, 0.17308781555585018)\n",
      "CBF1:ARG81: (-0.2192689594801929, -0.03363015605202929)\n",
      "CBF1:MET28: (0.08239014045383972, 0.21725137611340745)\n",
      "CBF1:SUT1: (0.0013204470995899372, 0.20979309187315287)\n",
      "CBF1:AZF1: (-0.15859562883092804, -0.01911590208260141)\n",
      "CBF1:GAL4: (0.08855249921344015, 0.30769402877789037)\n",
      "CBF1:MSN2: (0.09345201116818, 0.2768407901563067)\n",
      "Significant coefficients for 90.0, where intervals are entirely above or below ±0.0:\n",
      "CBF1:MET28: (0.0702883319201939, 0.21243861704095554)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# TODO: must have option to add max_lrb\n",
    "\n",
    "# TODO: the top10% models likely should not have teh same number of classes as the\n",
    "# all data, possibly not stratified at all\n",
    "\n",
    "# return at this point, for use later in the notebook, the response variable\n",
    "# and the all data stratification classes\n",
    "\n",
    "def get_significant_predictors(perturbed_tf, response_df, predictors_df, **kwargs):\n",
    "    \"\"\"\n",
    "    This function is used to get the significant predictors for a given TF. It is\n",
    "    capable of conducting steps 1.1 and 1.2 described above.\n",
    "\n",
    "    :params perturbed_tf: str, the TF for which the significant predictors are to be\n",
    "        identified\n",
    "    :params response_df: pd.DataFrame, the response dataframe containing the response\n",
    "        values\n",
    "    :params predictors_df: pd.DataFrame, the predictors dataframe containing the\n",
    "        predictor values\n",
    "    :params kwargs: dict, additional arguments to be passed to the function. Expected \n",
    "    arguments are 'quantile_threshold' fom generate_modeling_data() and 'ci_percentile'\n",
    "    from examine_bootstrap_coefficients()\n",
    "\n",
    "    :return sig_coef_dict: dict, a dictionary containing the significant predictors\n",
    "        and their corresponding coefficients\n",
    "    \"\"\"\n",
    "\n",
    "    y, X = generate_modeling_data(perturbed_tf,\n",
    "                                  response_df,\n",
    "                                  predictors_df,\n",
    "                                  quantile_threshold=kwargs.get(\"quantile_threshold\", None),\n",
    "                                  drop_intercept=True)\n",
    "\n",
    "    # NOTE: fit_intercept is set to `true`\n",
    "    lassoCV_estimator = LassoCV(\n",
    "        fit_intercept=True,\n",
    "        max_iter=10000,\n",
    "        selection=\"random\",\n",
    "        random_state=42,\n",
    "        n_jobs=4)\n",
    "    \n",
    "    predictor_variable = re.sub(r\"_rep\\d+\", \"\", perturbed_tf)\n",
    "\n",
    "    stratification_classes = stratification_classification(X[predictor_variable].squeeze(), y.squeeze())\n",
    "\n",
    "    # Fit the model to the data in order to extract the alphas_ which are generated\n",
    "    # during the fitting process\n",
    "    lasso_model = stratified_cv_modeling(\n",
    "        y, X, stratification_classes, lassoCV_estimator)\n",
    "\n",
    "    # set the alphas_ attribute of the lassoCV_estimator to the alphas_ attribute of the\n",
    "    # lasso_model fit on the whole data. This will allow the\n",
    "    # bootstrap_stratified_cv_modeling function to use the same set of lambdas\n",
    "    lassoCV_estimator.alphas_ = lasso_model.alphas_\n",
    "\n",
    "\n",
    "    # for test purposes, set n_bootstraps to 10\n",
    "    # NOTE: fit_intercept=True is passed to the internal Lasso model for bootstrap\n",
    "    # iterations, along with some other settings\n",
    "    \n",
    "    logging.info(\"running bootstraps\")\n",
    "    bootstrap_lasso_output = bootstrap_stratified_cv_modeling(\n",
    "        y=y,\n",
    "        X=X,\n",
    "        estimator=lassoCV_estimator,\n",
    "        ci_percentile=kwargs.get(\"ci_percentile\", 95.0),\n",
    "        n_bootstraps=kwargs.get(\"n_bootstraps\", 10),\n",
    "        max_iter=10000,\n",
    "        fit_intercept=True,\n",
    "        selection=\"random\",\n",
    "        random_state=42)\n",
    "\n",
    "    sig_coef_plt, sig_coef_dict = examine_bootstrap_coefficients(\n",
    "        bootstrap_lasso_output,\n",
    "        ci_level=kwargs.get(\"ci_percentile\", 95.0))\n",
    "    \n",
    "    plt.close(sig_coef_plt)\n",
    "    \n",
    "    return sig_coef_dict, y, stratification_classes\n",
    "\n",
    "all_data_sig_coef, all_y, all_stratification_classes = get_significant_predictors(\n",
    "    \"CBF1\",\n",
    "    response_df,\n",
    "    predictors_df,\n",
    "    ci_percentile=99.8,\n",
    "    n_bootstraps=100)\n",
    "\n",
    "top10_data_sig_coef, top10_y, top10_stratification_classes = get_significant_predictors(\n",
    "    \"CBF1\",\n",
    "    response_df,\n",
    "    predictors_df,\n",
    "    quantile_threshold=0.1,\n",
    "    ci_percentile=90.0,\n",
    "    n_bootstraps=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2\n",
    "\n",
    "We next need to intersect the significant coefficients (see definitions above) in both\n",
    "models. In this case, a single interactor survives (note that there are only 100\n",
    "bootstraps in this example in the interest of speed for the tutorial. We recommend no \n",
    "less than 1000 in practice)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The surviving coefficients are: {'CBF1:MET28'}\n"
     ]
    }
   ],
   "source": [
    "intersect_coefficients = set(all_data_sig_coef.keys()).intersection(set(top10_data_sig_coef.keys()))\n",
    "print(f\"The surviving coefficients are: {intersect_coefficients}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3\n",
    "\n",
    "We next implement the method which searches alternative models, which include the\n",
    "surviving interactor terms, with variations on including the main effect. In this case, \n",
    "we have only 1 term. But, we would do the following for each surviving interactor term,\n",
    "if there is more than one. The goal of this process, remember, is to generate a set of\n",
    "high confidence interactor terms for this TF. If the predictive power of the main effect\n",
    "is equivalent or better than a model with the interactor, we consider that a low\n",
    "confidence interactor effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The full model avg r-squared is 0.010517208487239915\n",
      "The interactor results are: []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_117397/3504430721.py:87: FutureWarning: Passing a set as an indexer is deprecated and will raise in a future version. Use a list instead.\n",
      "  full_X.loc[:,intersect_coefficients],\n",
      "/home/chase/code/yeastdnnexplorer/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:737: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=4.\n",
      "  warnings.warn(\n",
      "/home/chase/code/yeastdnnexplorer/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:737: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=4.\n",
      "  warnings.warn(\n",
      "/home/chase/code/yeastdnnexplorer/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:737: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=4.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def stratified_cv_r2(\n",
    "    y: pd.DataFrame,\n",
    "    X: pd.DataFrame,\n",
    "    classes: np.ndarray,\n",
    "    estimator: BaseEstimator = LinearRegression(), \n",
    "    skf: StratifiedKFold = StratifiedKFold(n_splits=4, shuffle=True, random_state=42)\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Calculate the average stratified CV r-squared for a given estimator and data. By\n",
    "    default, this is a 4-fold stratified CV with a LinearRegression estimator.\n",
    "\n",
    "    :param y: The response variable. See generate_modeling_data()\n",
    "    :param X: The predictor variables. See generate_modeling_data()\n",
    "    :param classes: the stratification classes for the data\n",
    "    :param estimator: the estimator to be used in the modeling. By default, this is a\n",
    "        LinearRegression() model.\n",
    "    :param skf: the StratifiedKFold object to be used in the modeling. By default, this\n",
    "        is a 4-fold stratified CV with shuffle=True and random_state=42.\n",
    "    \n",
    "    :return: the average r-squared value for the stratified CV\n",
    "    \"\"\"\n",
    "\n",
    "    estimator_local = clone(estimator)\n",
    "    r2_scores = []\n",
    "\n",
    "    for train_idx, test_idx in skf.split(X, classes):\n",
    "        # Use train and test indices to split X and y\n",
    "        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "        # Fit the model\n",
    "        model = estimator_local.fit(X_train, y_train)\n",
    "\n",
    "        # Calculate R-squared and append to r2_scores\n",
    "        r2_scores.append(r2_score(y_test, model.predict(X_test)))\n",
    "\n",
    "\n",
    "    return np.mean(r2_scores)\n",
    "\n",
    "\n",
    "def test_interactor_variants(\n",
    "        intersect_coefficients,\n",
    "        interactor,\n",
    "        **kwargs) -> dict[str | Union[str, float]]:\n",
    "    \"\"\"\n",
    "    For a given interactor, replace the term in the formula with two variants:\n",
    "        1. the main effect\n",
    "        2. the main effect + interactor\n",
    "    For each of the variants, calculate the average stratified CV r-squared with \n",
    "    stratified_cv_r2().\n",
    "\n",
    "    :param intersect_coefficients: the set of coefficients that are determined to be\n",
    "        significant, expected to be from either a bootstrap procedure on a LassoCV\n",
    "        model on a full partition of the data and the top 10% by perturbed binding, or\n",
    "        LassoCV followed by backwards selection by adj-rsquared.\n",
    "    :param interactor: the interactor term to be tested\n",
    "    :param kwargs: additional arguments to be passed to the function. Expected\n",
    "        arguments are 'y', 'X', and 'stratification_classes'. See stratified_cv_r2()\n",
    "        for more information.\n",
    "\n",
    "    :return: a list with three dict entries, each with key\n",
    "        'interactor', 'variant', 'avg_r2'\n",
    "    \"\"\"\n",
    "    y = kwargs.get(\"y\")\n",
    "    if y is None:\n",
    "        raise ValueError(\"y must be passed as a keyword argument\")\n",
    "    X = kwargs.get(\"X\")\n",
    "    if X is None:\n",
    "        raise ValueError(\"X must be passed as a keyword argument\")\n",
    "    stratification_classes = kwargs.get(\"stratification_classes\")\n",
    "    if stratification_classes is None:\n",
    "        raise ValueError(\"stratification_classes must be passed as a keyword argument\")\n",
    "    \n",
    "    main_effect = interactor.split(\":\")[1]\n",
    "    interactor_formula_variants = [main_effect, [main_effect, interactor]]\n",
    "\n",
    "    output = []\n",
    "    for variant in interactor_formula_variants:\n",
    "        # replace the interactor term in the formula with the variant\n",
    "        variant_predictors = ([term for term in intersect_coefficients if term != interactor] \n",
    "                              + [variant] if isinstance(variant, str) else variant)\n",
    "        # conduct the stratified CV r-squared calculation with the formula variant\n",
    "        input_model_avg_rsquared = stratified_cv_r2(\n",
    "            y, \n",
    "            X.loc[:,variant_predictors],\n",
    "            stratification_classes)\n",
    "        \n",
    "        # append the results to the output list\n",
    "        output.append({\n",
    "            \"interactor\": interactor,\n",
    "            \"variant\": variant,\n",
    "            \"avg_r2\": input_model_avg_rsquared\n",
    "        })\n",
    "        \n",
    "    return output\n",
    "\n",
    "def get_interactor_importance(\n",
    "        y: pd.DataFrame,\n",
    "        full_X: pd.DataFrame,\n",
    "        stratification_classes: np.ndarray,\n",
    "        intersect_coefficients: set,\n",
    "        )-> tuple[float, list[dict[str | Union[str, float]]]]:\n",
    "    \"\"\"\n",
    "    For each interactor in the intersect_coefficients, run test_interactor_importance\n",
    "    to compare the variants' avg_rsquared to the input_model_avg_rsquared. If a variant\n",
    "    of the interactor term is better, record it in a dictionary. Return the \n",
    "    `instersect_coefficient` model's avg R-squared and the dictionary of interaction\n",
    "    alternatives that, when that alternative replaces a single interaction term,\n",
    "    improves the rsquared.\n",
    "\n",
    "    :param y: the response variable\n",
    "    :param full_X: the full predictor matrix\n",
    "    :param stratification_classes: the stratification classes for the data\n",
    "    :param intersect_coefficients: the set of coefficients that are determined to be\n",
    "        significant, expected to be from either a bootstrap procedure on a LassoCV\n",
    "        model on a full partition of the data and the top 10% by perturbed binding, or\n",
    "        LassoCV followed by backwards selection by adj-rsquared.\n",
    "    \n",
    "    :return: a tuple with the first element being the input_model_avg_rsquared and the\n",
    "        second element being a list of dictionaries with keys 'interactor', 'variant',\n",
    "        and 'avg_r2'\n",
    "    \"\"\"\n",
    "    \n",
    "    input_model_avg_rsquared = stratified_cv_r2(\n",
    "        y,\n",
    "        full_X.loc[:,intersect_coefficients],\n",
    "        stratification_classes)\n",
    "\n",
    "    # for each interactor in the intersect_coefficients, run test_interactor_importance\n",
    "    # compare the variants' avg_rsquared to the input_model_avg_rsquared. Record\n",
    "    # the best performing.\n",
    "    interactor_results = []\n",
    "    for interactor in intersect_coefficients:\n",
    "        if \":\" in interactor:\n",
    "            \n",
    "            interactor_variant_results = test_interactor_variants(\n",
    "                intersect_coefficients,\n",
    "                interactor,\n",
    "                y=y,\n",
    "                X=full_X,\n",
    "                stratification_classes=stratification_classes)\n",
    "            \n",
    "            # compare the avg_r2 values of the two variants to input_model_avg_rsquared\n",
    "            variant_dict = max(interactor_variant_results, key=lambda x: x[\"avg_r2\"])\n",
    "            \n",
    "            if variant_dict[\"avg_r2\"] > input_model_avg_rsquared:\n",
    "                interactor_results.append(variant_dict)\n",
    "\n",
    "    return input_model_avg_rsquared, interactor_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conduct the analysis\n",
    "\n",
    "The functions above will be moved into source code once we settle one final forms.\n",
    "Below is how I imagine carrying out the analysis on a single TF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# get the additional main effects which will be tested from the intersect_coefficients\n",
    "main_effects = []\n",
    "for term in intersect_coefficients:\n",
    "    if \":\" in term:\n",
    "        main_effects.append(term.split(\":\")[1])\n",
    "    else:\n",
    "        main_effects.append(term)\n",
    "\n",
    "# combine these main effects with the intersect_coefficients\n",
    "interactor_terms_and_main_effects = list(intersect_coefficients) + main_effects\n",
    "\n",
    "# generate a model matrix with the intersect terms and the main effects. This full\n",
    "# model will not be used for modeling -- subsets of the columns will be, however.\n",
    "_, full_X = generate_modeling_data(\n",
    "    'CBF1',\n",
    "    response_df,\n",
    "    predictors_df,\n",
    "    formula = f\"~ {' + '.join(interactor_terms_and_main_effects)}\",\n",
    "    drop_intercept=False\n",
    ")\n",
    "\n",
    "# Currently, this function tests each interactor term in the intersect_coefficients\n",
    "# with two variants by replacing the interaction term with the main effect only, and\n",
    "# with the main effect + interactor. If either of the variants has a higher avg\n",
    "# r-squared than the intersect_model, then that variant is returned. In this case,\n",
    "# the original intersect_coefficients are the best model.\n",
    "full_avg_rsquared, x = get_interactor_importance(\n",
    "    all_y,\n",
    "    full_X,\n",
    "    all_stratification_classes,\n",
    "    intersect_coefficients\n",
    ")\n",
    "\n",
    "print(f\"The full model avg r-squared is {full_avg_rsquared}\")\n",
    "print(f\"The interactor results are: {x}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
