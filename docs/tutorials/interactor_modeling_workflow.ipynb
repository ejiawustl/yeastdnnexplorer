{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main changes: added a separate Workflow after the end of the first one. Altertered the try_interactor_variants method \n",
    "# in order to only substitute main effect and not add it\n",
    "\n",
    "# configure the logger to print to console\n",
    "from typing import Union\n",
    "import logging\n",
    "from matplotlib import pyplot as plt\n",
    "import re\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, clone\n",
    "from sklearn.linear_model import LassoCV, LinearRegression\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "\n",
    "\n",
    "from yeastdnnexplorer.ml_models.lasso_modeling import (\n",
    "    generate_modeling_data,\n",
    "    stratification_classification,\n",
    "    stratified_cv_modeling,\n",
    "    bootstrap_stratified_cv_modeling,\n",
    "    examine_bootstrap_coefficients,\n",
    "    get_significant_predictors,\n",
    "    stratified_cv_r2,\n",
    "    try_interactor_variants,\n",
    "    get_interactor_importance,\n",
    "    get_non_zero_predictors,\n",
    "    backwards_OLS_feature_selection,\n",
    "    get_full_data,\n",
    "    select_significant_features,\n",
    "    )\n",
    "\n",
    "\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "\n",
    "random.seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interactor Modeling Workflow 1\n",
    "\n",
    "This tutorial describes a process of modeling perturbation response by binding data\n",
    "with the goal of discovering a meaningful set of interactor terms. More specifically,\n",
    "we start with the following model:\n",
    "\n",
    "$$\n",
    "tf_{perturbed} \\sim tf_{perturbed} + tf_{perturbed}:tf_{2} + tf_{perturbed}:tf_{2} + ... + max(non\\ perturbed\\ binding)\n",
    "$$\n",
    "\n",
    "Where the response variable is the $tf_{perturbed}$ perturbation response, and the\n",
    "predictor variables are binding data (e.g., calling card experiments). Predictor\n",
    "terms such as $tf_{perturbed}:tf_{2}$ represent the interaction between the\n",
    "$tf_{perturbed}$ binding and the binding of another transcription factor. The final\n",
    "term, $\\max(\\text{non-perturbed binding})$, is defined as the maximum binding score\n",
    "for each gene, excluding $tf_{perturbed}$. This term is included to mitigate the\n",
    "effect of outlier genes which may have high binding scores across multiple\n",
    "transcription factors, potentially distorting the model.\n",
    "\n",
    "We assume that the actual relationship between the perturbation response and the\n",
    "binding data is sparse and use the following steps to identify significant terms.\n",
    "These terms represent a set of TFs which, when considered as interactors with the\n",
    "perturbed TF, improve the inferred relationship between the binding and perturbation\n",
    "data.\n",
    "\n",
    "\n",
    "## Interactor sparse modeling\n",
    "\n",
    "1. First, we apply bootstrapping to a 4-fold cross-validated Lasso model. The folds\n",
    "are stratified based on the binding data domain of the perturbed TF, ensuring that\n",
    "each fold better represents the domain structure.\n",
    "\n",
    "    - We produce two variations of this model:\n",
    "        \n",
    "        1. A model trained using all available data.\n",
    "        \n",
    "        2. A model trained using only the top 10% of data based on the binding\n",
    "        score of the perturbed TF.\n",
    "\n",
    "1. For model `1.1`, we select coefficients whose 99.8% confidence interval does not\n",
    "include zero. For model `1.2`, we select coefficients whose 90.0% confidence interval\n",
    "does not include zero. We assume that, due to the non-linear relationship between\n",
    "perturbation response and binding, interaction effects are more pronounced in the\n",
    "top 10% of the data. By intersecting the coefficients from both models, we highlight\n",
    "those that are predictive across the full dataset.\n",
    "\n",
    "1. Now, with our reduced set of features, we perform the exact same process as Step 3 in\n",
    "Workflow 1 above. That is,  With this set of predictors, next create an OLS model using the same 4-fold\n",
    "stratified cross validation from which we calculated an average $R^2$. Next, for each\n",
    "interactor in the model, we produce one other cross validated OLS model by\n",
    "replacing the interactor with its corresponding main effect. We note if this\n",
    "variant yields a better average $R^2$. We remove all features from our set in which the main effect outperforms the interactor term.\n",
    "\n",
    "1. Finally, we report, as significant interactors, the interaction terms which have survived the steps above. We use the average R-squared achieved by this model and compare it to the average R-squared achieved by the univariate counterpart (the response TF predicted solely by its main effect). We would like to create a boxplot of this comparison across all TFs to see how this pipeline affects model performance in explaining variance in contrast to the simple univariate model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***NOTE***: To generate the `response_df` and `predictors_df` below, see the first six \n",
    "cells in the LassoCV tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_df = pd.read_csv(\"~/htcf_local/lasso_bootstrap/erics_tfs/response_dataframe_20241105.csv\", index_col=0)\n",
    "predictors_df = pd.read_csv(\"~/htcf_local/lasso_bootstrap/erics_tfs/predictors_dataframe_20241105.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Find significant predictors using all of the data\n",
    "\n",
    "The function `get_significant_predictors()` is a wrapper of the lassoCV bootstrap \n",
    "protocol described in the LassoCV notebook. It allows using the same code to produce\n",
    "both the 'all data' (step 1.1) and 'top 10%' models (step 1.2), and returns the \n",
    "significant coefficients as described in the protocol above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Significant coefficients for 99.8, where intervals are entirely above or below ±0.0:\n",
      "CBF1:SWI6: (-0.15340683343647527, -0.0007913600573889841)\n",
      "CBF1:RGM1: (1.108602887592501e-05, 0.17429468895434033)\n",
      "CBF1:ARG81: (-0.21682807454185382, -0.04237904791794094)\n",
      "CBF1:MET28: (0.078854251438266, 0.18700460281512202)\n",
      "CBF1:AZF1: (-0.15333695359516117, -0.012929575918097505)\n",
      "CBF1:GAL4: (0.11671175479397583, 0.32225532638329824)\n",
      "CBF1:MSN2: (0.08410392831397877, 0.2548485159840825)\n",
      "max_lrb: (0.000988469681798027, 0.11028595405133462)\n",
      "Significant coefficients for 90.0, where intervals are entirely above or below ±0.0:\n",
      "CBF1:ARG81: (-0.16367301981599305, -0.018194961118955785)\n",
      "CBF1:MET28: (0.0808741229105233, 0.19464882619151444)\n",
      "CBF1:AZF1: (-0.08412597626524772, -0.0004959148887297946)\n",
      "CBF1:GAL4: (0.00914657465324337, 0.229843453877801)\n"
     ]
    }
   ],
   "source": [
    "all_data_sig_coef, all_y, all_stratification_classes = get_significant_predictors(\n",
    "    \"CBF1\",\n",
    "    response_df,\n",
    "    predictors_df,\n",
    "    ci_percentile=99.8,\n",
    "    n_bootstraps=100,\n",
    "    add_max_lrb=True)\n",
    "\n",
    "top10_data_sig_coef, top10_y, top10_stratification_classes = get_significant_predictors(\n",
    "    \"CBF1\",\n",
    "    response_df,\n",
    "    predictors_df,\n",
    "    quantile_threshold=0.1,\n",
    "    ci_percentile=90.0,\n",
    "    n_bootstraps=100,\n",
    "    add_max_lrb=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2\n",
    "\n",
    "We next need to intersect the significant coefficients (see definitions above) in both\n",
    "models. In this case, four interactors survive (note that there are only 100\n",
    "bootstraps in this example in the interest of speed for the tutorial. We recommend no \n",
    "less than 1000 in practice)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The surviving coefficients are: {'CBF1:MET28', 'CBF1:AZF1', 'CBF1:ARG81', 'CBF1:GAL4'}\n"
     ]
    }
   ],
   "source": [
    "intersect_coefficients = set(all_data_sig_coef.keys()).intersection(set(top10_data_sig_coef.keys()))\n",
    "print(f\"The surviving coefficients are: {intersect_coefficients}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3\n",
    "\n",
    "We next implement the method which searches alternative models, which include the\n",
    "surviving interactor terms, with variations on substituing in the main effect. In this case, \n",
    "we have 4 terms. Thus, we will do the following for each surviving interactor term. The goal of this process, remember, is to generate a set of\n",
    "high confidence interactor terms for this TF. If the predictive power of the main effect\n",
    "is equivalent or better than a model with the interactor, we consider that a low\n",
    "confidence interactor effect. \n",
    "\n",
    "After identifying all interactor terms for which substituting in the main effect improves\n",
    "the average R-squared from CV, we drop these terms from our set of features. We then log the final average R-squared achieved by this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ericjia/yeastdnnexplorer/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:737: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=4.\n",
      "  warnings.warn(\n",
      "/Users/ericjia/yeastdnnexplorer/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:737: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=4.\n",
      "  warnings.warn(\n",
      "/Users/ericjia/yeastdnnexplorer/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:737: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=4.\n",
      "  warnings.warn(\n",
      "/Users/ericjia/yeastdnnexplorer/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:737: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=4.\n",
      "  warnings.warn(\n",
      "/Users/ericjia/yeastdnnexplorer/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:737: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=4.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The full model avg r-squared is 0.03092194305599505\n",
      "The interactor results are: [{'interactor': 'CBF1:GAL4', 'variant': 'GAL4', 'avg_r2': 0.048032396899616414}]\n",
      "Removing term: CBF1:GAL4\n",
      "The final model avg r-squared is 0.017883902536610485\n",
      "Final set of terms: {'CBF1:MET28', 'CBF1:AZF1', 'CBF1:ARG81'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ericjia/yeastdnnexplorer/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:737: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=4.\n",
      "  warnings.warn(\n",
      "/Users/ericjia/yeastdnnexplorer/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:737: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=4.\n",
      "  warnings.warn(\n",
      "/Users/ericjia/yeastdnnexplorer/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:737: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=4.\n",
      "  warnings.warn(\n",
      "/Users/ericjia/yeastdnnexplorer/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:737: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=4.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# get the additional main effects which will be tested from the intersect_coefficients\n",
    "main_effects = []\n",
    "for term in intersect_coefficients:\n",
    "    if \":\" in term:\n",
    "        main_effects.append(term.split(\":\")[1])\n",
    "    else:\n",
    "        main_effects.append(term)\n",
    "\n",
    "# combine these main effects with the intersect_coefficients\n",
    "interactor_terms_and_main_effects = list(intersect_coefficients) + main_effects\n",
    "\n",
    "# generate a model matrix with the intersect terms and the main effects. This full\n",
    "# model will not be used for modeling -- subsets of the columns will be, however.\n",
    "_, full_X = generate_modeling_data(\n",
    "    'CBF1',\n",
    "    response_df,\n",
    "    predictors_df,\n",
    "    formula = f\"~ {' + '.join(interactor_terms_and_main_effects)}\",\n",
    "    drop_intercept=False,\n",
    ")\n",
    "\n",
    "full_X[\"max_lrb\"] = predictors_df.drop(columns=\"CBF1\").max(axis=1)\n",
    "\n",
    "# Currently, this function tests each interactor term in the intersect_coefficients\n",
    "# with two variants by replacing the interaction term with the main effect only, and\n",
    "# with the main effect + interactor. If either of the variants has a higher avg\n",
    "# r-squared than the intersect_model, then that variant is returned. In this case,\n",
    "# the original intersect_coefficients are the best model.\n",
    "full_avg_rsquared, x = get_interactor_importance(\n",
    "    all_y,\n",
    "    full_X,\n",
    "    all_stratification_classes,\n",
    "    intersect_coefficients\n",
    ")\n",
    "\n",
    "print(f\"The full model avg r-squared is {full_avg_rsquared}\")\n",
    "print(f\"The interactor results are: {x}\")\n",
    "\n",
    "# Now that we have identifed the interactors whose main effects improve average R-squared, we drop them from our model\n",
    "\n",
    "if x:\n",
    "    interactors_to_remove = set()\n",
    "    for dictionary in x:\n",
    "        interactor = dictionary.get(\"interactor\")\n",
    "        interactors_to_remove.add(interactor)  \n",
    "        print(\"Removing term: \"+str(interactor))\n",
    "\n",
    "    final_feature_set = intersect_coefficients.difference(interactors_to_remove)\n",
    "    # get the final avg r-squared for this set\n",
    "    final_model_avg_r_squared, _ =get_interactor_importance(\n",
    "        all_y,\n",
    "        full_X,\n",
    "        all_stratification_classes,\n",
    "        final_feature_set\n",
    "    )\n",
    "\n",
    "else:\n",
    "    final_feature_set = intersect_coefficients\n",
    "    final_model_avg_r_squared = full_avg_rsquared\n",
    "\n",
    "print(f\"The final model avg r-squared is {final_model_avg_r_squared}\")\n",
    "print(f\"Final set of terms: {final_feature_set}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Comparing our final model to a univariate model\n",
    "\n",
    "In our last step, we take our reamining set of features from the end of Step 3, and now compare its performance to that of a univariate model where the response TF is predicted solely by its main effect. We will use the average R-squared achieved by 4-Fold CV on both models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The univariate average R-squared is: 0.004970412632932103\n",
      "The final model average R-squared is 0.017883902536610485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ericjia/yeastdnnexplorer/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:737: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=4.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "y, X = generate_modeling_data(\"CBF1\", response_df, predictors_df, drop_intercept=True, formula=\"CBF1_LRR ~ CBF1\")\n",
    "classes = stratification_classification(X[\"CBF1\"].squeeze(), y.squeeze())\n",
    "avg_r2_univariate = stratified_cv_r2(\n",
    "        y,\n",
    "        X,  \n",
    "        classes,\n",
    "    )\n",
    "\n",
    "print(f\"The univariate average R-squared is: {avg_r2_univariate}\")\n",
    "print(f\"The final model average R-squared is {final_model_avg_r_squared}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the final mdel we achieved through Workflow 1 demonstrates a higher average R-squared achieved by 4-fold CV. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interactor Modeling Workflow 2\n",
    "\n",
    "An alternative workflow to identifying a meaningful set of transcription factors takes a slightly different approach. There are some commonalities between this workflow and Workflow 1, and we will point them out in the steps below which outline how this alternative workflow operates.\n",
    "\n",
    "\n",
    "1. First, we apply a 4-fold cross-validated Lasso model (without bootstrapping - this is the \n",
    "difference between this step and Step 1 from Workflow 1). The folds\n",
    "are stratified based on the binding data domain of the perturbed TF, ensuring that\n",
    "each fold better represents the domain structure.\n",
    "\n",
    "    - We produce two variations of this model:\n",
    "        \n",
    "        1. A model trained using all available data.\n",
    "        \n",
    "        2. A model trained using only the top 10% of data based on the binding\n",
    "        score of the perturbed TF.\n",
    "\n",
    "2. Each Lasso model from Step 1 will return a set of non-zero coefficients. We then intersect the coefficients from both models, and retain this list of features\n",
    "in the exact same fashion as Step 2 of Workflow 1.  \n",
    "\n",
    "3. With this set of predictors, we then perform a \"backwards OLS feature selection,\" in \n",
    "which we create OLS models using this set of predictors on the entire dataset, and remove\n",
    "features which have a p-value above a threshold for significance (0.001). We continously \n",
    "remove features and re-create models on the reduced set of features until all of the \n",
    "features in a model are significant. Then, taking this filtered set of predictors, we \n",
    "perform the same process, but this time on the top 10% of the data based on the binding\n",
    "score of the perturbed TF. Since we are now using a smaller dataset, our threshold for \n",
    "significance is increased (0.01), and we perform the same process until we arrive at a \n",
    "final model in which all of the terms are significant. \n",
    "\n",
    "From here onwards, we follow the same steps as Step 3 and Step 4 in Workflow 1 above. I have copied their descriptions from above, and have renamed them Steps 4 and 5 to match the numbering for this workflow.\n",
    "\n",
    "4. Now, with our reduced set of features, we perform the exact same process as Step 3 in\n",
    "Workflow 1 above. That is,  With this set of predictors, next create an OLS model using the same 4-fold\n",
    "stratified cross validation from which we calculated an average $R^2$. Next, for each\n",
    "interactor in the model, we produce one other cross validated OLS model by\n",
    "replacing the interactor with its corresponding main effect. We note if this\n",
    "variant yields a better average $R^2$. We remove all features from our set in which the main effect outperforms the interactor term.\n",
    "\n",
    "5. Finally, we report, as significant interactors, the interaction terms which have survived the steps above. We use the average R-squared achieved by this model and compare it to the average R-squared achieved by the univariate counterpart (the response TF predicted solely by its main effect). We would like to create a boxplot of this comparison across all TFs to see how this pipeline affects model performance in explaining variance in contrast to the simple univariate model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's choose a particular TF to run though this workflow with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_of_interest = \"CBF1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: get the non-zero coefficients from the Lasso models\n",
    "\n",
    "The function `get_non_zero_predictors()` is a wrapper of the stratified CV modeling \n",
    "protocol described in the LassoCV notebook. It allows using the same code to produce\n",
    "the Lasso models trained on the 'all data' (step 1.1) and 'top 10%' models (step 1.2), and returns the \n",
    "features with non-zero coefficients as described in the protocol above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1.1: get the non-zero features on all data\n",
    "tf_surviving_terms = get_non_zero_predictors(tf_of_interest, response_df, predictors_df)\n",
    "# Step 1.2: get the non-zero features on only the top10% of genes\n",
    "tf_top10_surviving_terms = get_non_zero_predictors(tf_of_interest, response_df, predictors_df, quantile_threshold=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Intersect the features from both Lasso models\n",
    "\n",
    "Here, we simply intersect the sets of non-zero features found by both of the Lasso models in Step 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The surviving coefficients are: {'CBF1:MET28', 'CBF1:SKN7', 'CBF1:SWI6', 'CBF1:AZF1'}\n"
     ]
    }
   ],
   "source": [
    "intersect_coefficients = set(tf_surviving_terms).intersection(set(tf_top10_surviving_terms))\n",
    "print(f\"The surviving coefficients are: {intersect_coefficients}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Perform backwards OLS feature selection on the intersected features\n",
    "\n",
    "Now, taking our set of intersecting features from the step above, we perform the process of backwards OLS feature selection as described above. The function `backwards_OLS_feature_selection()` is a wrapper function that repeatedly calls `select_significant_features()` to perform the iterative process of removing insignificant features based on their p-value with respect to the given threshold. It also uses a helper method called `get_full_data()` to transform the input data into a single DataFrame that is usable by Patsy to generate design matrices for these OLS models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "backward_OLS_feature_result = backwards_OLS_feature_selection(tf_of_interest, intersect_coefficients, response_df, predictors_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4\n",
    "\n",
    "This is now the exact same workflow as Step 3 from Workflow 1. To recap, we now implement the method which searches alternative models, which include the surviving interactor terms, with variations on including the main effect. The goal of this process is to generate a set of high confidence interactor terms for this TF. If the predictive power of the main effect is equivalent or better than a model with the interactor, we consider that a low confidence interactor effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The full model avg r-squared is 0.016951766474153418\n",
      "The interactor results are: []\n",
      "The final model avg r-squared is 0.016951766474153418\n",
      "Final set of terms: {'CBF1:MET28', 'CBF1:SKN7', 'CBF1:SWI6', 'CBF1:AZF1'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ericjia/yeastdnnexplorer/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:737: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=4.\n",
      "  warnings.warn(\n",
      "/Users/ericjia/yeastdnnexplorer/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:737: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=4.\n",
      "  warnings.warn(\n",
      "/Users/ericjia/yeastdnnexplorer/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:737: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=4.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# get the additional main effects which will be tested from the backward_OLS_feature_result\n",
    "main_effects = []\n",
    "for term in backward_OLS_feature_result:\n",
    "    if \":\" in term:\n",
    "        main_effects.append(term.split(\":\")[1])\n",
    "    else:\n",
    "        main_effects.append(term)\n",
    "\n",
    "# combine these main effects with the backward_OLS_feature_result\n",
    "interactor_terms_and_main_effects = list(backward_OLS_feature_result) + main_effects\n",
    "\n",
    "# generate a model matrix with the intersect terms and the main effects. This full\n",
    "# model will not be used for modeling -- subsets of the columns will be, however.\n",
    "_, full_X = generate_modeling_data(\n",
    "    tf_of_interest,\n",
    "    response_df,\n",
    "    predictors_df,\n",
    "    formula = f\"~ {' + '.join(interactor_terms_and_main_effects)}\",\n",
    "    drop_intercept=False\n",
    ")\n",
    "\n",
    "# add the max_lrb term to the data\n",
    "full_X[\"max_lrb\"] = predictors_df.drop(columns=\"CBF1\").max(axis=1)\n",
    "\n",
    "# have to generate the stratification classes and the \"y\" column for input into \n",
    "# get_interactor_importance below\n",
    "y, X = generate_modeling_data(tf_of_interest, response_df, predictors_df)\n",
    "all_stratification_classes = stratification_classification(X[tf_of_interest].squeeze(), y.squeeze())\n",
    "\n",
    "# Currently, this function tests each interactor term in the backward_OLS_feature_result\n",
    "# with two variants by replacing the interaction term with the main effect only, and\n",
    "# with the main effect + interactor. If either of the variants has a higher avg\n",
    "# r-squared than the intersect_model, then that variant is returned. \n",
    "full_avg_rsquared, x = get_interactor_importance(\n",
    "    y,\n",
    "    full_X,\n",
    "    all_stratification_classes,\n",
    "    backward_OLS_feature_result\n",
    ")\n",
    "\n",
    "print(f\"The full model avg r-squared is {full_avg_rsquared}\")\n",
    "print(f\"The interactor results are: {x}\")\n",
    "\n",
    "if x:\n",
    "    interactors_to_remove = set()\n",
    "    for dictionary in x:\n",
    "        interactor = dictionary.get(\"interactor\")\n",
    "        interactors_to_remove.add(interactor)  \n",
    "        print(f\"Removing term: {interactor}\")\n",
    "\n",
    "    final_feature_set = intersect_coefficients.difference(interactors_to_remove)\n",
    "    # get the final avg r-squared for this set\n",
    "    final_model_avg_r_squared, _ =get_interactor_importance(\n",
    "        y,\n",
    "        full_X,\n",
    "        all_stratification_classes,\n",
    "        final_feature_set\n",
    "    )\n",
    "\n",
    "else:\n",
    "    final_feature_set = intersect_coefficients\n",
    "    final_model_avg_r_squared = full_avg_rsquared\n",
    "\n",
    "print(f\"The final model avg r-squared is {final_model_avg_r_squared}\")\n",
    "print(f\"Final set of terms: {final_feature_set}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5\n",
    "\n",
    "This is the same as Step 4 from Workflow 1 above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The univariate average R-squared is: 0.004970412632932103\n",
      "The final model average R-squared is 0.016951766474153418\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ericjia/yeastdnnexplorer/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:737: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=4.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "y, X = generate_modeling_data(tf_of_interest, response_df, predictors_df, drop_intercept=True, formula=f\"{tf_of_interest}_LRR ~ {tf_of_interest}\")\n",
    "classes = stratification_classification(X[tf_of_interest].squeeze(), y.squeeze())\n",
    "avg_r2_univariate = stratified_cv_r2(\n",
    "        y,\n",
    "        X,  \n",
    "        classes,\n",
    "    )\n",
    "\n",
    "print(f\"The univariate average R-squared is: {avg_r2_univariate}\")\n",
    "print(f\"The final model average R-squared is {final_model_avg_r_squared}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the final mdel we achieved through Workflow 2 demonstrates a higher average R-squared achieved by 4-fold CV. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
