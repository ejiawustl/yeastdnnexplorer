{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure the logger to print to console\n",
    "import logging\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.linear_model import LassoCV, LinearRegression\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "\n",
    "\n",
    "from yeastdnnexplorer.ml_models.lasso_modeling import (\n",
    "    generate_modeling_data,\n",
    "    stratification_classification,\n",
    "    stratified_cv_modeling,\n",
    "    bootstrap_stratified_cv_modeling,\n",
    "    examine_bootstrap_coefficients)\n",
    "\n",
    "\n",
    "logging.basicConfig(level=logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interactor Modeling Workflow\n",
    "\n",
    "This tutorial describes a process of modeling perturbation response by binding data\n",
    "with the goal of discovering a meaningful set of interactor terms. More specifically,\n",
    "we start with the following model:\n",
    "\n",
    "$$\n",
    "tf_{perturbed} \\sim tf_{perturbed} + tf_{perturbed}:tf_{2} + tf_{perturbed}:tf_{2} + ... + max(non\\ perturbed\\ binding)\n",
    "$$\n",
    "\n",
    "Where the response variable is the $tf_{perturbed}$ perturbation response, and the\n",
    "predictor variables are binding data (e.g., calling card experiments). Predictor\n",
    "terms such as $tf_{perturbed}:tf_{2}$ represent the interaction between the\n",
    "$tf_{perturbed}$ binding and the binding of another transcription factor. The final\n",
    "term, $\\max(\\text{non-perturbed binding})$, is defined as the maximum binding score\n",
    "for each gene, excluding $tf_{perturbed}$. This term is included to mitigate the\n",
    "effect of outlier genes which may have high binding scores across multiple\n",
    "transcription factors, potentially distorting the model.\n",
    "\n",
    "We assume that the actual relationship between the perturbation response and the\n",
    "binding data is sparse and use the following steps to identify significant terms.\n",
    "These terms represent a set of TFs which, when considered as interactors with the\n",
    "perturbed TF, improve the inferred relationship between the binding and perturbation\n",
    "data.\n",
    "\n",
    "\n",
    "## Interactor sparse modeling\n",
    "\n",
    "1. First, we apply bootstrapping to a 4-fold cross-validated Lasso model. The folds\n",
    "are stratified based on the binding data domain of the perturbed TF, ensuring that\n",
    "each fold better represents the domain structure.\n",
    "\n",
    "    - We produce two variations of this model:\n",
    "        \n",
    "        1. A model trained using all available data.\n",
    "        \n",
    "        2. A model trained using only the top 10% of data based on the binding\n",
    "        score of the perturbed TF.\n",
    "\n",
    "1. For model `1.1`, we select coefficients whose 99.8% confidence interval does not\n",
    "include zero. For model `1.2`, we select coefficients whose 90.0% confidence interval\n",
    "does not include zero. We assume that, due to the non-linear relationship between\n",
    "perturbation response and binding, interaction effects are more pronounced in the\n",
    "top 10% of the data. By intersecting the coefficients from both models, we highlight\n",
    "those that are predictive across the full dataset.\n",
    "\n",
    "1. With this set of predictors, next create an OLS model using the same 4-fold\n",
    "stratified cross validation from which we calculated an average $R^2$. Next, for each\n",
    "interactor in the model, we produce two other cross validated OLS models, one by\n",
    "replacing the interactor with its corresponding main effect, and another that\n",
    "includes both the interaction term and the main effect. We note which of these\n",
    "variants yields the best average $R^2$.\n",
    "\n",
    "1. Finally, we report, as significant interactors, those interaction terms which, when \n",
    "retained in the model, improve the $R^2$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***NOTE***: To generate the `response_df` and `predictors_df` below, see the first six \n",
    "cells in the LassoCV tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_df = pd.read_csv(\"~/htcf_local/lasso_bootstrap/erics_tfs/response_dataframe_20241105.csv\", index_col=0)\n",
    "predictors_df = pd.read_csv(\"~/htcf_local/lasso_bootstrap/erics_tfs/predictors_dataframe_20241105.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Find significant predictors using all of the data\n",
    "\n",
    "The function `get_significant_predictors()` is a wrapper of the lassoCV bootstrap \n",
    "protocol described in the LassoCV notebook. It allows using the same code to produce\n",
    "both the 'all data' (step 1.1) and 'top 10%' models (step 1.2), and returns the \n",
    "significant coefficients as described in the protocol above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO: must have option to add max_lrb\n",
    "\n",
    "# TODO: the top10% models likely should not have teh same number of classes as the\n",
    "# all data, possibly not stratified at all\n",
    "\n",
    "# return at this point, for use later in the notebook, the response variable\n",
    "# and the all data stratification classes\n",
    "\n",
    "def get_significant_predictors(perturbed_tf, response_df, predictors_df, **kwargs):\n",
    "    \"\"\"\n",
    "    This function is used to get the significant predictors for a given TF. It is\n",
    "    capable of conducting steps 1.1 and 1.2 described above.\n",
    "\n",
    "    :params perturbed_tf: str, the TF for which the significant predictors are to be\n",
    "        identified\n",
    "    :params response_df: pd.DataFrame, the response dataframe containing the response\n",
    "        values\n",
    "    :params predictors_df: pd.DataFrame, the predictors dataframe containing the\n",
    "        predictor values\n",
    "    :params kwargs: dict, additional arguments to be passed to the function. Expected \n",
    "    arguments are 'quantile_threshold' fom generate_modeling_data() and 'ci_percentile'\n",
    "    from examine_bootstrap_coefficients()\n",
    "\n",
    "    :return sig_coef_dict: dict, a dictionary containing the significant predictors\n",
    "        and their corresponding coefficients\n",
    "    \"\"\"\n",
    "\n",
    "    y, X = generate_modeling_data(perturbed_tf,\n",
    "                                  response_df,\n",
    "                                  predictors_df,\n",
    "                                  quantile_threshold=kwargs.get(\"quantile_threshold\", None),\n",
    "                                  drop_intercept=True)\n",
    "\n",
    "    # NOTE: fit_intercept is set to `true`\n",
    "    lassoCV_estimator = LassoCV(\n",
    "        fit_intercept=True,\n",
    "        max_iter=10000,\n",
    "        selection=\"random\",\n",
    "        random_state=42,\n",
    "        n_jobs=4)\n",
    "\n",
    "    # Fit the model to the data in order to extract the alphas_ which are generated\n",
    "    # during the fitting process\n",
    "    lasso_model = stratified_cv_modeling(y, X, lassoCV_estimator)\n",
    "\n",
    "    # set the alphas_ attribute of the lassoCV_estimator to the alphas_ attribute of the\n",
    "    # lasso_model fit on the whole data. This will allow the\n",
    "    # bootstrap_stratified_cv_modeling function to use the same set of lambdas\n",
    "    lassoCV_estimator.alphas_ = lasso_model.alphas_\n",
    "\n",
    "\n",
    "    # for test purposes, set n_bootstraps to 10\n",
    "    # NOTE: fit_intercept=True is passed to the internal Lasso model for bootstrap\n",
    "    # iterations, along with some other settings\n",
    "    \n",
    "    logging.info(\"running bootstraps\")\n",
    "    bootstrap_lasso_output = bootstrap_stratified_cv_modeling(\n",
    "        y=y,\n",
    "        X=X,\n",
    "        estimator=lassoCV_estimator,\n",
    "        ci_percentile=kwargs.get(\"ci_percentile\", 95.0),\n",
    "        n_bootstraps=kwargs.get(\"n_bootstraps\", 10),\n",
    "        max_iter=10000,\n",
    "        fit_intercept=True,\n",
    "        selection=\"random\",\n",
    "        random_state=42)\n",
    "\n",
    "    sig_coef_plt, sig_coef_dict = examine_bootstrap_coefficients(\n",
    "        bootstrap_lasso_output,\n",
    "        ci_level=kwargs.get(\"ci_percentile\", 95.0))\n",
    "    \n",
    "    plt.close(sig_coef_plt)\n",
    "    \n",
    "    return sig_coef_dict\n",
    "\n",
    "all_data_sig_coef = get_significant_predictors(\n",
    "    \"CBF1\",\n",
    "    response_df,\n",
    "    predictors_df,\n",
    "    ci_percentile=99.8,\n",
    "    n_bootstraps=100)\n",
    "\n",
    "top10_data_sig_coef = get_significant_predictors(\n",
    "    \"CBF1\",\n",
    "    response_df,\n",
    "    predictors_df,\n",
    "    quantile_threshold=0.1,\n",
    "    ci_percentile=90.0,\n",
    "    n_bootstraps=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2\n",
    "\n",
    "We next need to intersect the significant coefficients (see definitions above) in both\n",
    "models. In this case, a single interactor survives (note that there are only 100\n",
    "bootstraps in this example in the interest of speed for the tutorial. We recommend no \n",
    "less than 1000 in practice)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The surviving coefficients are: {'CBF1:MET28'}\n"
     ]
    }
   ],
   "source": [
    "intersect_coefficients = set(all_data_sig_coef.keys()).intersection(set(top10_data_sig_coef.keys()))\n",
    "print(f\"The surviving coefficients are: {intersect_coefficients}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3\n",
    "\n",
    "We next implement the method which searches alternative models, which include the\n",
    "surviving interactor terms, with variations on including the main effect. In this case, \n",
    "we have only 1 term. But, we would do the following for each surviving interactor term,\n",
    "if there is more than one. The goal of this process, remember, is to generate a set of\n",
    "high confidence interactor terms for this TF. If the predictive power of the main effect\n",
    "is equivalent or better than a model with the interactor, we consider that a low\n",
    "confidence interactor effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:main:Removing CBF1 from the data rows (removing the perturbed TF)\n",
      "INFO:main:Number of rows in the merged response/predictors: 6149\n",
      "INFO:main:Generating modeling data with formula: CBF1_LRR ~ CBF1:MET28 + CBF1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.010517208487239943\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chase/code/yeastdnnexplorer/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:737: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=4.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def stratified_cv_r_squared(\n",
    "    y: pd.DataFrame,\n",
    "    X: pd.Dataframe,\n",
    "    classes: list,\n",
    "    estimator: BaseEstimator = LinearRegression(), \n",
    "    skf: StratifiedKFold = StratifiedKFold(n_splits=4, shuffle=True, random_state=42),\n",
    "    **kwargs\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "\n",
    "    estimator_local = clone(estimator)\n",
    "    r2_scores = []\n",
    "\n",
    "    for train_idx, test_idx in skf.split(X, classes):\n",
    "        # Use train and test indices to split X and y\n",
    "        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "        # Fit the model\n",
    "        model = estimator_local.fit(X_train, y_train)\n",
    "\n",
    "        # Predict on the test set\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        # Calculate R-squared and append to r2_scores\n",
    "        r2_scores.append(r2_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "    return np.mean(r2_scores)\n",
    "\n",
    "# NOTE! I am adding the main effect to the model b/c it is needed in\n",
    "# `stratified_cv_r_squared`\n",
    "# it is subsequencint\n",
    "input_model_formula = f\"CBF1_LRR ~ {' + '.join(intersect_coefficients)} + CBF1\"\n",
    "\n",
    "input_model_y, input_model_X = generate_modeling_data(\n",
    "    'CBF1',\n",
    "    response_df,\n",
    "    predictors_df,\n",
    "    formula = input_model_formula,\n",
    "    drop_intercept=False\n",
    ")\n",
    "\n",
    "# TODO: for chase -- I am going to refractor the way the lasso fucntions work so that\n",
    "# they also take 'classes' as an argument. \n",
    "classes = stratification_classification(input_model_X['CBF1'].squeeze(), input_model_y['CBF1'].squeeze())\n",
    "\n",
    "input_model_X = input_model_X.drop(columns=['CBF1'], axis=1)\n",
    "\n",
    "input_model_avg_rsquared = stratified_cv_r_squared(\n",
    "    input_model_y,\n",
    "    input_model_X,\n",
    "    classes)\n",
    "\n",
    "print(input_model_avg_rsquared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up a function that iterates over iteractors and compares the 'main_effect' and\n",
    "# 'main_effect + interactor' models' avg r-squared values to the input_model_avg_rsquared.\n",
    "# return value is going to be a dictionary of predictors where the value denotes significance?\n",
    "\n",
    "def get_high_confidence_interactors(\n",
    "        response_variable,\n",
    "        predictors,\n",
    "        response_df,\n",
    "        predictors_df):\n",
    "\n",
    "    \n",
    "    # at this point, for loop over the predictors. If the predictor is an interaction\n",
    "    # term, then run a model where it is replaced by the main effect and replaced with\n",
    "    # the main effect + interactor. After this is done (in the loop)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
